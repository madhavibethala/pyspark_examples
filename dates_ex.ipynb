{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dates_ex.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjg39BrpuL5n9wDVKq6y/i",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madhavibethala/pyspark_examples/blob/main/dates_ex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEnAv4BRkrFU",
        "outputId": "959a73bd-4df4-48f4-9a5e-be3fc52293fe"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [66.2 kB]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:13 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Ign:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [695 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:16 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,263 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [544 kB]\n",
            "Get:19 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [510 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,421 kB]\n",
            "Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,786 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,196 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [39.4 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,699 kB]\n",
            "Get:27 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [914 kB]\n",
            "Get:28 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [40.9 kB]\n",
            "Get:29 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [44.1 kB]\n",
            "Fetched 13.6 MB in 8s (1,632 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXYdHo9ulHzC"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWRsFGn-lLg4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "a57ec6c3-dbba-40dd-b13b-f8edb391cfc1"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext\n",
        "sc=SparkContext.getOrCreate()\n",
        "sc"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://0b9b2ce380fe:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s0MzpSRlN5O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "a7c18441-eb64-4956-f55d-40be99c727da"
      },
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.getOrCreate()\n",
        "spark"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://0b9b2ce380fe:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f68e4a27210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy8XpoCnlTm1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb71d09-2096-4039-9bac-c568158f53e3"
      },
      "source": [
        "!ls\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  spark-2.3.1-bin-hadoop2.7\tspark-2.3.1-bin-hadoop2.7.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k19jLTVK0vZ"
      },
      "source": [
        "from pyspark.sql.functions import to_date,to_timestamp,lit,col\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6uZHp4v-Zyn",
        "outputId": "6aedce0a-44f7-4ea4-9c0c-25efbde1daeb"
      },
      "source": [
        "df=spark.createDataFrame([('2019-12-25 13:30:00',)],['Christmas'])\n",
        "df.show(1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+\n",
            "|          Christmas|\n",
            "+-------------------+\n",
            "|2019-12-25 13:30:00|\n",
            "+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR6Fqwp8_KXK",
        "outputId": "01285158-3cbe-4211-d8d8-ce523fe503d3"
      },
      "source": [
        "df.select(to_date(col('Christmas'),'yyyy:MM:dd HH:mm:ss')).show(1)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------------------------------+\n",
            "|to_date(`Christmas`, 'yyyy:MM:dd HH:mm:ss')|\n",
            "+-------------------------------------------+\n",
            "|                                       null|\n",
            "+-------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye7agzZb_qRm",
        "outputId": "17970644-3a59-4ebd-8089-13c1fcd20d83"
      },
      "source": [
        "df.select(to_date(col('Christmas'),'yyyy-MM-dd HH:mm:ss'),to_timestamp(col('Christmas'),'yyyy-MM-dd HH:mm:ss')).show(1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------------------------------+------------------------------------------------+\n",
            "|to_date(`Christmas`, 'yyyy-MM-dd HH:mm:ss')|to_timestamp(`Christmas`, 'yyyy-MM-dd HH:mm:ss')|\n",
            "+-------------------------------------------+------------------------------------------------+\n",
            "|                                 2019-12-25|                             2019-12-25 13:30:00|\n",
            "+-------------------------------------------+------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XPgMQHsAVOs",
        "outputId": "9725331b-567b-49ef-91dc-bf22118d320d"
      },
      "source": [
        "df1=spark.createDataFrame([('25/Dec/2009 13:30:00',)],['Christmas'])\n",
        "df1.show(1)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|           Christmas|\n",
            "+--------------------+\n",
            "|25/Dec/2009 13:30:00|\n",
            "+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4knj1Vs5CfJn",
        "outputId": "efd01e81-67c4-4ff9-8027-a6991c71cfd3"
      },
      "source": [
        "df1.select(to_date(col('Christmas'),'dd/MMM/yyyy HH:mm:ss'),to_timestamp(col('Christmas'),'dd/MMM/yyyy HH:mm:ss')).show(1)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------+-------------------------------------------------+\n",
            "|to_date(`Christmas`, 'dd/MMM/yyyy HH:mm:ss')|to_timestamp(`Christmas`, 'dd/MMM/yyyy HH:mm:ss')|\n",
            "+--------------------------------------------+-------------------------------------------------+\n",
            "|                                  2009-12-25|                              2009-12-25 13:30:00|\n",
            "+--------------------------------------------+-------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKC962BkC6qz",
        "outputId": "9f70b8b6-7456-486e-b610-9dbe2cd9241c"
      },
      "source": [
        "df1=spark.createDataFrame([('12/25/2009 13:30:00',)],['Christmas'])\n",
        "df1.show(1)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+\n",
            "|          Christmas|\n",
            "+-------------------+\n",
            "|12/25/2009 13:30:00|\n",
            "+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN_jtyScEMPc",
        "outputId": "68119d4b-d8ca-4dad-e998-63ebe099f883"
      },
      "source": [
        "df1.select(to_date(col('Christmas'),'MM/dd/yyyy HH:mm:ss'),to_timestamp(col('Christmas'),'MM/dd/yyyy HH:mm:ss')).show(1)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------------------------------+------------------------------------------------+\n",
            "|to_date(`Christmas`, 'MM/dd/yyyy HH:mm:ss')|to_timestamp(`Christmas`, 'MM/dd/yyyy HH:mm:ss')|\n",
            "+-------------------------------------------+------------------------------------------------+\n",
            "|                                 2009-12-25|                             2009-12-25 13:30:00|\n",
            "+-------------------------------------------+------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "Snkf1mcpEd2-",
        "outputId": "551cd491-2364-4114-ba0d-ca70319567b9"
      },
      "source": [
        "from pyspark.sql import SparkSession \n",
        "spark=SparkSession.builder.getOrCreate()\n",
        "df=spark.createDataFrame()\n",
        "df.display()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-40b7296fae83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: createDataFrame() missing 1 required positional argument: 'data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhgrqvL-ADyF",
        "outputId": "81fc7536-c9c5-4500-baed-4b8ab33d981b"
      },
      "source": [
        "from pyspark.sql import * \n",
        "\n",
        "spark=SparkSession.builder.getOrCreate()\n",
        "passanger=Row('Name','age','source','destination')\n",
        "passanger1=passanger('David',22,'London','Paris')\n",
        "passanger2=passanger('Steve',22,'New York','Sydey')\n",
        "passanger_data=[passanger1,passanger2]\n",
        "df=spark.createDataFrame(passanger_data)\n",
        "df.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+--------+-----------+\n",
            "| Name|age|  source|destination|\n",
            "+-----+---+--------+-----------+\n",
            "|David| 22|  London|      Paris|\n",
            "|Steve| 22|New York|      Sydey|\n",
            "+-----+---+--------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_o8UUsUFXir",
        "outputId": "c16ae3e4-13fd-4304-e828-1850481b81ae"
      },
      "source": [
        "from pyspark.sql import SQLContext\n",
        "sqlContext = SQLContext(sc)\n",
        "from pyspark.sql.functions import rand, randn\n",
        "df = sqlContext.range(0, 7)\n",
        "df.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "|  5|\n",
            "|  6|\n",
            "+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBaEEUaUNw9J",
        "outputId": "23839b9a-9cc5-4a62-fe73-f1a77fabdb32"
      },
      "source": [
        "\n",
        "new_data=df.select('id',rand(seed=10).alias ('uniform'),rand(seed=27).alias ('normal')).show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------------------+-------------------+\n",
            "| id|            uniform|             normal|\n",
            "+---+-------------------+-------------------+\n",
            "|  0|0.41371264720975787|  0.714105256846827|\n",
            "|  1| 0.7311719281896606| 0.8143487574232506|\n",
            "|  2| 0.9031701155118229| 0.5282207324381174|\n",
            "|  3| 0.1982919638208397|0.19369846818250636|\n",
            "|  4|0.12714181165849525| 0.0838940132767162|\n",
            "|  5| 0.7604318153406678| 0.9895050223719429|\n",
            "|  6|   0.83487085888236| 0.8394775938142013|\n",
            "+---+-------------------+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAK5IxbVtp1r",
        "outputId": "19d701cd-5938-4ee4-f359-d2379d3973ce"
      },
      "source": [
        "df = sqlContext.range(0, 10).withColumn('rand1', rand(seed=10)).withColumn('rand2', rand(seed=27))\n",
        "df.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------------------+-------------------+\n",
            "| id|              rand1|              rand2|\n",
            "+---+-------------------+-------------------+\n",
            "|  0|0.41371264720975787|  0.714105256846827|\n",
            "|  1| 0.7311719281896606| 0.8143487574232506|\n",
            "|  2| 0.9031701155118229| 0.5282207324381174|\n",
            "|  3|0.09430205113458567| 0.4420100497826609|\n",
            "|  4|0.38340505276222947| 0.9387162206758006|\n",
            "|  5| 0.1982919638208397|0.19369846818250636|\n",
            "|  6|0.12714181165849525| 0.0838940132767162|\n",
            "|  7| 0.7604318153406678| 0.9895050223719429|\n",
            "|  8|   0.83487085888236| 0.8394775938142013|\n",
            "|  9| 0.3142596916968412| 0.5809927918049477|\n",
            "+---+-------------------+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT59eINmv6it",
        "outputId": "3f1c7ca9-23c2-4ef3-aa8f-e5bd3ea8af15"
      },
      "source": [
        "df.select([mean(\"rand1\"), min(\"rand1\"), max(\"rand1\")]).show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+-------------------+------------------+\n",
            "|        avg(rand1)|         min(rand1)|        max(rand1)|\n",
            "+------------------+-------------------+------------------+\n",
            "|0.4760757936207261|0.09430205113458567|0.9031701155118229|\n",
            "+------------------+-------------------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JLUdP4Wx0-l",
        "outputId": "b6ab88ae-a90e-45c5-f36c-c6a0744a2918"
      },
      "source": [
        "df.describe('rand1','rand2').show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-------------------+------------------+\n",
            "|summary|              rand1|             rand2|\n",
            "+-------+-------------------+------------------+\n",
            "|  count|                 10|                10|\n",
            "|   mean| 0.4760757936207261|0.6124968906616971|\n",
            "| stddev| 0.3055791028722139|0.3060712918225637|\n",
            "|    min|0.09430205113458567|0.0838940132767162|\n",
            "|    max| 0.9031701155118229|0.9895050223719429|\n",
            "+-------+-------------------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGmUjRpMyHaV",
        "outputId": "97945f5d-2985-46cf-8f9a-a1871f32ace6"
      },
      "source": [
        "df.stat.cov('rand1','rand2')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05983805032757693"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzKWwqpp0g4H",
        "outputId": "d97c1175-88dc-46b4-ada1-c93880b82b4a"
      },
      "source": [
        "df.stat.corr('rand1','rand2')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6397807763656534"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "5rbvWew-0sNs",
        "outputId": "1a1237e9-8222-4a74-ef8d-6587851dfc4e"
      },
      "source": [
        "names=['madhu','jai','syam']\n",
        "items=[\"milk\", \"bread\", \"butter\", \"apples\", \"oranges\"]\n",
        "#df.spark.createDataFrame([(names[i%3]),(items[i%5]) for i in range(100)],['name','item'])\n",
        "df = sqlContext.createDataFrame([(names[i%3], (items[i % 5]) for i in range(100)], [\"name\", \"item\"])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-eb237aa32370>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    df = sqlContext.createDataFrame([(names[i%3], (items[i % 5]) for i in range(100)], [\"name\", \"item\"])\u001b[0m\n\u001b[0m                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "iTRdG0FX1w1-",
        "outputId": "e5dddadc-745a-4805-f77d-08cf908dc818"
      },
      "source": [
        "# Put your code here\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.getOrCreate()\n",
        "spark"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://9d96ea3da67b:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f4872fdaa90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "5zkU1vtt56zM",
        "outputId": "c5c6bde7-ccba-4beb-c3f7-c867cc0a2e64"
      },
      "source": [
        "wget https://downloads.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n",
        "tar xvf spark-*\n",
        "sudo mv spark-3.0.3-bin-hadoop2.7 /opt/spark\n",
        "echo \"export SPARK_HOME=/opt/spark\" >> ~/.bashrc\n",
        "echo \"export PATH=$PATH:$SPARK_HOME/bin\" >> ~/.bashrc\n",
        "echo \"export PYSPARK_PYTHON=/usr/bin/python3\" >> ~/.bashrc\n",
        "rm -rf spark-3.0.3-bin-hadoop2.7.tgz\n",
        "python3 -m pip install pytest-spark \n",
        "python3 -m pip install pyspark\n",
        "python3 -m pip install pytest\n",
        "echo done"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-42-2d8b7977b309>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    wget https://downloads.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}